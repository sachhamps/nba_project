\relax 
\citation{DeLay2016}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction to the NBA}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Project Aim and Motivation}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces An Example of an NBA Box Score showing the stats of the Oklahoma City Thunder players who started the game\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction to Machine Learning Models}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Features and Feature Selection Methods}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Training and Test Data}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Overfitting}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A visualisation of overfitting. The red line represents the error of the testing data whereas the blue line represents the training error. If there is an attempt to completely minimise the training data's error, then the model will become less flexible when attempting to predict future data points, resulting in an increase in the testing data's error. \relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:my-label}{{1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Dataset}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Overview of the Features}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table consisting of all the players in the dataset\relax }}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Table explaining the box score stats\relax }}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Technical Background}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Simple Moving Average}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Simple Linear Regression}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example of Simple Linear Regression. The magenta line in Figure 1b) visualises the linear relationship between two variables. It can be seen that the two variables have a positive correlation\relax }}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Bayesian Linear Regression}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Kernel Ridge Regression}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Different values of $\lambda $ has a large impact on the fit of a model. Figure 3a) implies that the model has learnt the training data too well leading to an overfit. Figure 3b) better fits the data points.\relax }}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces By applying a basis function, the data points are mapped from a 1-dimensional feature space to a 2-dimensional feature space, where it can be linearly separated\relax }}{11}}
\newlabel{fig:my-label}{{4}{11}}
\citation{DeLay2016}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Multiple Linear Regression and Multivariable Kernel Ridge Regression}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Root Mean Squared Error}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Existing Literature}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}ARIMA model Time Series Player Data \cite  {DeLay2016}}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Weibull-Gamma Statistical Model}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Application to the NBA Dataset}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  The Figure above visualise the relationship between the points scored of a player in the dataset over the course of their career's so far. The blue line shows the regression represents the linear relationship between the two variables and the shaded blue region represents one standard deviation. \relax }}{15}}
\newlabel{fig:my-label}{{5}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Feature Selection}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}SelectKBest Algorithm}{16}}
\newlabel{fig:1}{{6a}{17}}
\newlabel{sub@fig:1}{{a}{17}}
\newlabel{fig:2}{{6b}{17}}
\newlabel{sub@fig:2}{{b}{17}}
\newlabel{fig:3}{{6c}{17}}
\newlabel{sub@fig:3}{{c}{17}}
\newlabel{fig:1}{{6d}{17}}
\newlabel{sub@fig:1}{{d}{17}}
\newlabel{fig:2}{{6e}{17}}
\newlabel{sub@fig:2}{{e}{17}}
\newlabel{fig:3}{{6f}{17}}
\newlabel{sub@fig:3}{{f}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The Figures above visualise the relationship between points scored in a game and specific stats from the prior game. This was done to see if there was a positive correlation between these features and the target variable. The Figures shown are taken from one player in the dataset\relax }}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Feature Importance Using the Extra-Trees Classifier}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The bar charts show two different iterations of the extra-tree classifier for one player. When attempting to choose the 4 most correlated features, some features were selected in one iteration and not selected in another. As a result, the models were applied with both sets of features to see which one resulted in the lowest RMSE\relax }}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Motivation for the Machine Learning Models}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Simple Moving Average Model}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Simple Linear Regression}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Bayesian Linear Regression}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Kernel Ridge Regression}{19}}
\newlabel{fig:1}{{8a}{20}}
\newlabel{sub@fig:1}{{a}{20}}
\newlabel{fig:2}{{8b}{20}}
\newlabel{sub@fig:2}{{b}{20}}
\newlabel{fig:3}{{8c}{20}}
\newlabel{sub@fig:3}{{c}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The relationship between the two variables are not linear, therefore, attempting to use a linear regression would result in a poor fit of the data observed in b). Kernel ridge regression produces a non-linear curve which better fits the relationship between the two variables, seen in c)\relax }}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.5}Multiple Linear Regression and Multivariable Kernel Ridge Regression}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments and Results}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parameter Tuning}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Choosing $n$ for the Simple Moving Average Model}{22}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The different RMSE values when using different number of $n$ for predicting the number of points for every game of the season. As the table shows, using a window of 20 games produces the lowest testing error.\relax }}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Picking the correct Regularisation parameter}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Choosing Correct window for form}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  The RMSE of the testing data for Kernel Ridge Regression with different regularisation values\relax }}{23}}
\newlabel{fig:my-label}{{9}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The different RMSE values when using different window sizes\relax }}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results when attempting to forecast the points scored for every player on a game by game basis\relax }}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Results when attempting to forecast season average points per game in the 18/19 season\relax }}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Simple Moving Average}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Simple and Bayesian Linear Regression}{24}}
\newlabel{fig:1}{{10a}{25}}
\newlabel{sub@fig:1}{{a}{25}}
\newlabel{fig:2}{{10b}{25}}
\newlabel{sub@fig:2}{{b}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces These Figures show the results of the SMA model. The blue data points represents the training data points. The orange line depicts the prediction of the SMA model. The orange data points are the points actually scored by the players in the 18/19 season\relax }}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Kernel Ridge Regression}{25}}
\newlabel{fig:1}{{11a}{26}}
\newlabel{sub@fig:1}{{a}{26}}
\newlabel{fig:2}{{11b}{26}}
\newlabel{sub@fig:2}{{b}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces These Figures show the results of the simple linear regression model. The blue data points represents the training data points. The blue line depicts the best fitting line of the training data and the orange line is the predicted points scored for every game in the 2018/19 season. The orange data points are the points actually scored by the players in the 18/19 season\relax }}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Multivariable Linear Regression and Kernel Ridge Regression}{26}}
\newlabel{fig:1}{{12a}{27}}
\newlabel{sub@fig:1}{{a}{27}}
\newlabel{fig:2}{{12b}{27}}
\newlabel{sub@fig:2}{{b}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces These Figures depict the results produced from the Kernel Ridge Regression model for two players in the dataset, Nikola Jokic and Zach Lavine.\relax }}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Discussion and Analysis}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Forecasting Player Points Per Game for Every Game}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Forecasting Season Average Points Per Game}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Comparing the Models}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Limitations in the Models}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Overfitting Problem}{29}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Comparing the RMSE of the training data and the testing data for every model when attempting to forecast the points scored for players for every game.\relax }}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Comparing the RMSE of the training data and the testing data for every model when attempting to forecast the average points scored per game over the course of the season\relax }}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Limitations}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions and Future Work}{32}}
